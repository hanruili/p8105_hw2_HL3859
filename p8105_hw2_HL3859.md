p8105_hw2_HL3859
================
Hanrui Li
2024-09-25

# Problem 1

Import and clean the data; convert the entry variable to logical:

``` r
library(tidyverse)

nyc = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |>
  janitor::clean_names()

nyc_clean = 
  nyc |>
  select(line, station_name, station_latitude, station_longitude, route1:route11, 
         entry, vending, entrance_type, ada) |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))

nyc_clean
```

    ## # A tibble: 1,868 × 19
    ##    line     station_name station_latitude station_longitude route1 route2 route3
    ##    <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ##  1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  6 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  7 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  8 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  9 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ## 10 4 Avenue 53rd St                  40.6             -74.0 R      <NA>   <NA>  
    ## # ℹ 1,858 more rows
    ## # ℹ 12 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <dbl>, route9 <dbl>, route10 <dbl>, route11 <dbl>, entry <lgl>,
    ## #   vending <chr>, entrance_type <chr>, ada <lgl>

The dataset contains `line` which shows 36 subway lines. `station_name`
represents the names of the station. `station_latitude` and
`station_longitude` represent the coordinates of the station. `route1`
to `route11` show the subway routes served by the station, there are 11
routes. `entry` indicates whether entrance is allowed (TRUE/FALSE).
`vending` indicates whether there is a vending machine (YES/NO).
`entrance_type` is the type of entrance (e.g. Stair, Elevator, Easement,
Escalator, Door, Ramp, Walkway). `ada` inndicates whether the station is
compliant with ADA standards (TRUE/FALSE). I selected relevant columns
from the dataset and converted the `entry` variable to a logical
variable for easier analysis. After cleaning, the dataset has 1868 rows
and 19 columns. This dataset is tidy as each column represents a
variable and each row represents a single observation (an
entrance/exit).

``` r
nyc_clean |>
  distinct(line, station_name) |>
  nrow()
```

    ## [1] 465

- There are **465** distinct stations.

``` r
nyc_clean |>
  filter(ada == "TRUE") |>
  distinct(line, station_name) |>
  nrow()
```

    ## [1] 84

- **84** stations are ADA compliant.

``` r
nyc_clean |>
  filter(vending == "NO") |>
  summarise(proportion = mean(entry))
```

    ## # A tibble: 1 × 1
    ##   proportion
    ##        <dbl>
    ## 1      0.377

``` r
entry_true = 
  nyc_clean |>
  filter(vending == "NO" & entry == TRUE) |>
  nrow()

vending_no = 
  nyc_clean |>
  filter(vending == "NO") |>
  nrow()

entry_true / vending_no
```

    ## [1] 0.3770492

- **37.70%** of station entrances / exits without vending allow
  entrance.

Reformat data so that route number and route name are distinct
variables:

``` r
nyc_routes = 
  nyc_clean |>
  mutate(across(starts_with("route"), as.character)) |>
  pivot_longer(
    route1:route11,
    names_to = "route_name", 
    values_to = "route_number") |>
  filter(!is.na(route_number))

nyc_routes
```

    ## # A tibble: 4,270 × 10
    ##    line     station_name station_latitude station_longitude entry vending
    ##    <chr>    <chr>                   <dbl>             <dbl> <lgl> <chr>  
    ##  1 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ##  2 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ##  3 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  4 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  5 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  6 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  7 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  8 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ##  9 4 Avenue 45th St                  40.6             -74.0 TRUE  YES    
    ## 10 4 Avenue 45th St                  40.6             -74.0 TRUE  YES    
    ## # ℹ 4,260 more rows
    ## # ℹ 4 more variables: entrance_type <chr>, ada <lgl>, route_name <chr>,
    ## #   route_number <chr>

``` r
nyc_routes |>
  filter(route_number == "A") |>
  distinct(line, station_name) |>
  nrow()
```

    ## [1] 60

- There are **60** distinct stations serve the A train.

``` r
nyc_routes |>
  filter(route_number == "A", ada == "TRUE") |>
  distinct(line, station_name) |>
  nrow()
```

    ## [1] 17

- Of the stations that serve the A train, **17** are ADA compliant.

# Problem 2

Import, clean, and tidy the data for Mr. Trash Wheel:

``` r
library(readxl)
library(dplyr)

trash_wheel = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", skip = 1) |>
  janitor::clean_names() |>
  select(dumpster, month, year, date, weight_tons, volume_cubic_yards, plastic_bottles, polystyrene,
         cigarette_butts, glass_bottles, plastic_bags, wrappers, 
         sports_balls, homes_powered) |>
  filter(!is.na(dumpster)) |>
  mutate(sports_balls = as.integer(round(sports_balls)), 
         homes_powered = (weight_tons * 500 / 30),
         trash_wheel = "Mr. Trash Wheel")

trash_wheel
```

    ## # A tibble: 651 × 15
    ##    dumpster month year  date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ##  1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ##  2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ##  3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ##  4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ##  5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ##  6        6 May   2014  2014-05-20 00:00:00        2.71                 13
    ##  7        7 May   2014  2014-05-21 00:00:00        1.91                  8
    ##  8        8 May   2014  2014-05-28 00:00:00        3.7                  16
    ##  9        9 June  2014  2014-06-05 00:00:00        2.52                 14
    ## 10       10 June  2014  2014-06-11 00:00:00        3.76                 18
    ## # ℹ 641 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

Import, clean, and tidy the data for Professor Trash Wheel:

``` r
prof_trash_wheel = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", skip = 1) |>
  janitor::clean_names() |>
  select(dumpster, month, year, date, weight_tons, volume_cubic_yards, plastic_bottles,
         polystyrene, cigarette_butts, glass_bottles, plastic_bags, wrappers, homes_powered) |>
  filter(!is.na(dumpster)) |>
  mutate(year = as.character(year),
         trash_wheel = "Professor Trash Wheel")
  
prof_trash_wheel
```

    ## # A tibble: 119 × 14
    ##    dumpster month    year  date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>    <chr> <dttm>                    <dbl>              <dbl>
    ##  1        1 January  2017  2017-01-02 00:00:00        1.79                 15
    ##  2        2 January  2017  2017-01-30 00:00:00        1.58                 15
    ##  3        3 February 2017  2017-02-26 00:00:00        2.32                 18
    ##  4        4 February 2017  2017-02-26 00:00:00        3.72                 15
    ##  5        5 February 2017  2017-02-28 00:00:00        1.45                 15
    ##  6        6 March    2017  2017-03-30 00:00:00        1.71                 15
    ##  7        7 April    2017  2017-04-01 00:00:00        1.82                 15
    ##  8        8 April    2017  2017-04-20 00:00:00        2.37                 15
    ##  9        9 May      2017  2017-05-10 00:00:00        2.64                 15
    ## 10       10 May      2017  2017-05-26 00:00:00        2.78                 15
    ## # ℹ 109 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>, trash_wheel <chr>

Import, clean, and tidy the data for Gwynnda:

``` r
gwynnda_trash_wheel = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1) |>
  janitor::clean_names() |>
  select(dumpster, month, year, date, weight_tons, volume_cubic_yards, plastic_bottles,
         polystyrene, cigarette_butts, plastic_bags, wrappers, homes_powered) |>
  filter(!is.na(dumpster)) |>
  mutate(year = as.character(year),
         trash_wheel = "Gwynnda Trash Wheel")

gwynnda_trash_wheel
```

    ## # A tibble: 263 × 13
    ##    dumpster month  year  date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>  <chr> <dttm>                    <dbl>              <dbl>
    ##  1        1 July   2021  2021-07-03 00:00:00        0.93                 15
    ##  2        2 July   2021  2021-07-07 00:00:00        2.26                 15
    ##  3        3 July   2021  2021-07-07 00:00:00        1.62                 15
    ##  4        4 July   2021  2021-07-16 00:00:00        1.76                 15
    ##  5        5 July   2021  2021-07-30 00:00:00        1.53                 15
    ##  6        6 August 2021  2021-08-11 00:00:00        2.06                 15
    ##  7        7 August 2021  2021-08-14 00:00:00        1.9                  15
    ##  8        8 August 2021  2021-08-16 00:00:00        2.16                 15
    ##  9        9 August 2021  2021-08-16 00:00:00        2.6                  15
    ## 10       10 August 2021  2021-08-17 00:00:00        3.21                 15
    ## # ℹ 253 more rows
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>, trash_wheel <chr>

Combine the datasets and produce a single tidy dataset:

``` r
combine = 
  bind_rows(trash_wheel, prof_trash_wheel, gwynnda_trash_wheel)

combine
```

    ## # A tibble: 1,033 × 15
    ##    dumpster month year  date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ##  1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ##  2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ##  3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ##  4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ##  5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ##  6        6 May   2014  2014-05-20 00:00:00        2.71                 13
    ##  7        7 May   2014  2014-05-21 00:00:00        1.91                  8
    ##  8        8 May   2014  2014-05-28 00:00:00        3.7                  16
    ##  9        9 June  2014  2014-06-05 00:00:00        2.52                 14
    ## 10       10 June  2014  2014-06-11 00:00:00        3.76                 18
    ## # ℹ 1,023 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, trash_wheel <chr>

The **Mr. Trash Wheel** dataset contains key variables such as dumpster
number, collection date (e.g., 2014-05-16), trash weight (e.g., 4.31
tons), volume in cubic yards, and various types of collected debris,
including plastic bottles, polystyrene, cigarette butts, and sports
balls. The **Professor Trash Wheel** dataset captures similar
information like the date (2017-01-02), total weight of trash collected
(e.g., 1.79 tons), and volume in cubic yards, alongside plastic bottles
and cigarette butts. The **Gwynnda Trash Wheel** dataset provides
detailed information on the trash collection efforts from this trash
wheel in Baltimore’s Inner Harbor. Key variables include the dumpster
number, the month, year, and exact date of each collection (e.g.,
2021-07-03), the total weight of trash collected in tons (e.g., 0.93
tons), and the volume of trash in cubic yards. It also tracks specific
types of debris, including plastic bottles, polystyrene, cigarette
butts, plastic bags, and wrappers.

The **combined** dataset from the Mr. Trash Wheel, Professor Trash
Wheel, and Gwynnda Trash Wheel includes a total of **1033**
observations, each representing a dumpster filled with trash removed
from Baltimore’s Inner Harbor. Key variables include the date (`date`,
e.g. 2014-05-16), the weight of trash collected (`weight_tons)`,
e.g. 4.31 tons), and the number of sports balls recovered
(`sports_balls`, e.g. 7). Additionally, the dataset tracks the number of
cigarette butts collected (`cigarette_butts`, e.g. 1.26^{5}).

For Professor Trash Wheel, the total weight of trash collected across
all available records is **246.74** tons.

In June 2022, Gwynnda collected a total of **1.812^{4}** cigarette
butts.

# Problem 3

Import, clean, and tidy the datasets:

``` r
library(tidyverse)

bakers =
  read_csv("data/gbb_datasets/bakers.csv") |>
  janitor::clean_names() |>
  mutate(baker = word(baker_name, 1, sep = " "),
         series = as.numeric(series))

bakes = 
  read_csv("data/gbb_datasets/bakes.csv") |>
  janitor::clean_names() |>
  mutate(series = as.numeric(series),
         episode = as.numeric(episode))

results = 
  read_csv("data/gbb_datasets/results.csv", skip = 2) |>
  janitor::clean_names() |>
  mutate(series = as.numeric(series),
         episode = as.numeric(episode))
```

Use `anti_join` to compare the datasets and check for completeness and
correctness across datasets:

``` r
anti_join(bakers, bakes, by = c("series", "baker"))
```

    ## # A tibble: 26 × 6
    ##    baker_name          series baker_age baker_occupation          hometown baker
    ##    <chr>                <dbl>     <dbl> <chr>                     <chr>    <chr>
    ##  1 Alice Fevronia          10        28 Geography teacher         Essex    Alice
    ##  2 Amelia LeBruin          10        24 Fashion designer          Halifax  Amel…
    ##  3 Antony Amourdoux         9        30 Banker                    London   Anto…
    ##  4 Briony Williams          9        33 Full-time parent          Bristol  Brio…
    ##  5 Dan Beasley-Harling      9        36 Full-time parent          London   Dan  
    ##  6 Dan Chambers            10        32 Support worker            Rotherh… Dan  
    ##  7 David Atherton          10        36 International health adv… Whitby   David
    ##  8 Helena Garcia           10        40 Online project manager    Leeds    Hele…
    ##  9 Henry Bird              10        20 Student                   Durham   Henry
    ## 10 Imelda McCarron          9        33 Countryside recreation o… County … Imel…
    ## # ℹ 16 more rows

``` r
anti_join(bakes, bakers, by = c("series", "baker"))
```

    ## # A tibble: 8 × 5
    ##   series episode baker    signature_bake                            show_stopper
    ##    <dbl>   <dbl> <chr>    <chr>                                     <chr>       
    ## 1      2       1 "\"Jo\"" Chocolate Orange CupcakesOrange and Card… Chocolate a…
    ## 2      2       2 "\"Jo\"" Caramelised Onion, Gruyere and Thyme Qui… Raspberry a…
    ## 3      2       3 "\"Jo\"" Stromboli flavored with Mozzarella, Ham,… Unknown     
    ## 4      2       4 "\"Jo\"" Lavender Biscuits                         Blueberry M…
    ## 5      2       5 "\"Jo\"" Salmon and Asparagus Pie                  Apple and R…
    ## 6      2       6 "\"Jo\"" Rum and Raisin Baked Cheesecake           Limoncello …
    ## 7      2       7 "\"Jo\"" Raspberry & Strawberry Mousse Cake        Pain Aux Ra…
    ## 8      2       8 "\"Jo\"" Raspberry and Blueberry Mille Feuille     Mini Victor…

``` r
anti_join(bakers, results, by = c("series", "baker"))
```

    ## # A tibble: 1 × 6
    ##   baker_name  series baker_age baker_occupation hometown     baker
    ##   <chr>        <dbl>     <dbl> <chr>            <chr>        <chr>
    ## 1 Jo Wheatley      2        41 Housewife        Ongar, Essex Jo

``` r
anti_join(results, bakers, by = c("series", "baker"))
```

    ## # A tibble: 8 × 5
    ##   series episode baker  technical result    
    ##    <dbl>   <dbl> <chr>      <dbl> <chr>     
    ## 1      2       1 Joanne        11 IN        
    ## 2      2       2 Joanne        10 IN        
    ## 3      2       3 Joanne         1 IN        
    ## 4      2       4 Joanne         8 IN        
    ## 5      2       5 Joanne         6 IN        
    ## 6      2       6 Joanne         1 STAR BAKER
    ## 7      2       7 Joanne         3 IN        
    ## 8      2       8 Joanne         1 WINNER

``` r
anti_join(bakes, results, by = c("series", "episode", "baker"))
```

    ## # A tibble: 8 × 5
    ##   series episode baker    signature_bake                            show_stopper
    ##    <dbl>   <dbl> <chr>    <chr>                                     <chr>       
    ## 1      2       1 "\"Jo\"" Chocolate Orange CupcakesOrange and Card… Chocolate a…
    ## 2      2       2 "\"Jo\"" Caramelised Onion, Gruyere and Thyme Qui… Raspberry a…
    ## 3      2       3 "\"Jo\"" Stromboli flavored with Mozzarella, Ham,… Unknown     
    ## 4      2       4 "\"Jo\"" Lavender Biscuits                         Blueberry M…
    ## 5      2       5 "\"Jo\"" Salmon and Asparagus Pie                  Apple and R…
    ## 6      2       6 "\"Jo\"" Rum and Raisin Baked Cheesecake           Limoncello …
    ## 7      2       7 "\"Jo\"" Raspberry & Strawberry Mousse Cake        Pain Aux Ra…
    ## 8      2       8 "\"Jo\"" Raspberry and Blueberry Mille Feuille     Mini Victor…

``` r
anti_join(results, bakes, by = c("series", "episode", "baker"))
```

    ## # A tibble: 596 × 5
    ##    series episode baker    technical result
    ##     <dbl>   <dbl> <chr>        <dbl> <chr> 
    ##  1      1       2 Lea             NA <NA>  
    ##  2      1       2 Mark            NA <NA>  
    ##  3      1       3 Annetha         NA <NA>  
    ##  4      1       3 Lea             NA <NA>  
    ##  5      1       3 Louise          NA <NA>  
    ##  6      1       3 Mark            NA <NA>  
    ##  7      1       4 Annetha         NA <NA>  
    ##  8      1       4 Jonathan        NA <NA>  
    ##  9      1       4 Lea             NA <NA>  
    ## 10      1       4 Louise          NA <NA>  
    ## # ℹ 586 more rows

Merge to create a single, combined dataset:

``` r
merged = 
  bakes |>
  full_join(bakers, by = c("baker" = "baker", "series" = "series")) |>
  full_join(results, by = c("baker" = "baker", "series" = "series", "episode" = "episode")) |>
  arrange(series, episode, baker)

write_csv(merged, "data/gbb_datasets/merged.csv")
```

**Data cleaning process:** I import the datasets and standardize column
names by converting them to lowercase and replacing spaces with
underscores. I extract the first name of the baker because first names
are sufficient to identify bakers, and I create a new column `baker` to
ensure the consistency across variable names. I also convert `series`
and `episode` to numeric format to ensure consistency across datasets
when merging. I check the completeness of the datasets using
`anti_join()`. I choose `full_join()` to merge `bakers`, `bakes`, and
`results` as all information from the three datasets can be included
even if some fields are missing.

**Final data discussion:** The final `merged` dataset includes
information on each baker, their bakes, and their performance in each
episode, along with details such as the `series`, `episode`, and
`result`. Each row corresponds to a unique combination of `series`,
`episode`, and `baker`. The dataset contains variables like the baker’s
name, the series and episode they participated in, the bake they
completed, and their result (such as `STAR BAKER` or `ELIMINATED`).

Create a table showing the star baker or winner of each episode in
Seasons 5 through 10:

``` r
star_bakers =
  merged |>
  filter(series >= 5 & result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, baker, result) |>
  arrange(series, episode)

knitr::kable(star_bakers)
```

| series | episode | baker     | result     |
|-------:|--------:|:----------|:-----------|
|      5 |       1 | Nancy     | STAR BAKER |
|      5 |       2 | Richard   | STAR BAKER |
|      5 |       3 | Luis      | STAR BAKER |
|      5 |       4 | Richard   | STAR BAKER |
|      5 |       5 | Kate      | STAR BAKER |
|      5 |       6 | Chetna    | STAR BAKER |
|      5 |       7 | Richard   | STAR BAKER |
|      5 |       8 | Richard   | STAR BAKER |
|      5 |       9 | Richard   | STAR BAKER |
|      5 |      10 | Nancy     | WINNER     |
|      6 |       1 | Marie     | STAR BAKER |
|      6 |       2 | Ian       | STAR BAKER |
|      6 |       3 | Ian       | STAR BAKER |
|      6 |       4 | Ian       | STAR BAKER |
|      6 |       5 | Nadiya    | STAR BAKER |
|      6 |       6 | Mat       | STAR BAKER |
|      6 |       7 | Tamal     | STAR BAKER |
|      6 |       8 | Nadiya    | STAR BAKER |
|      6 |       9 | Nadiya    | STAR BAKER |
|      6 |      10 | Nadiya    | WINNER     |
|      7 |       1 | Jane      | STAR BAKER |
|      7 |       2 | Candice   | STAR BAKER |
|      7 |       3 | Tom       | STAR BAKER |
|      7 |       4 | Benjamina | STAR BAKER |
|      7 |       5 | Candice   | STAR BAKER |
|      7 |       6 | Tom       | STAR BAKER |
|      7 |       7 | Andrew    | STAR BAKER |
|      7 |       8 | Candice   | STAR BAKER |
|      7 |       9 | Andrew    | STAR BAKER |
|      7 |      10 | Candice   | WINNER     |
|      8 |       1 | Steven    | STAR BAKER |
|      8 |       2 | Steven    | STAR BAKER |
|      8 |       3 | Julia     | STAR BAKER |
|      8 |       4 | Kate      | STAR BAKER |
|      8 |       5 | Sophie    | STAR BAKER |
|      8 |       6 | Liam      | STAR BAKER |
|      8 |       7 | Steven    | STAR BAKER |
|      8 |       8 | Stacey    | STAR BAKER |
|      8 |       9 | Sophie    | STAR BAKER |
|      8 |      10 | Sophie    | WINNER     |
|      9 |       1 | Manon     | STAR BAKER |
|      9 |       2 | Rahul     | STAR BAKER |
|      9 |       3 | Rahul     | STAR BAKER |
|      9 |       4 | Dan       | STAR BAKER |
|      9 |       5 | Kim-Joy   | STAR BAKER |
|      9 |       6 | Briony    | STAR BAKER |
|      9 |       7 | Kim-Joy   | STAR BAKER |
|      9 |       8 | Ruby      | STAR BAKER |
|      9 |       9 | Ruby      | STAR BAKER |
|      9 |      10 | Rahul     | WINNER     |
|     10 |       1 | Michelle  | STAR BAKER |
|     10 |       2 | Alice     | STAR BAKER |
|     10 |       3 | Michael   | STAR BAKER |
|     10 |       4 | Steph     | STAR BAKER |
|     10 |       5 | Steph     | STAR BAKER |
|     10 |       6 | Steph     | STAR BAKER |
|     10 |       7 | Henry     | STAR BAKER |
|     10 |       8 | Steph     | STAR BAKER |
|     10 |       9 | Alice     | STAR BAKER |
|     10 |      10 | David     | WINNER     |

Richard (season 5), Nadiya (season 6), Candice (season 7), Sophie
(season 8), Rahul (season 9), and Steph (season 10) are the overall
predictable winners. Nancy (season 5) is a surprise because Nancy only
wins twice compared to Richard’s 5 winning titles, and she is the final
winner of the season. David (season 10) is also a surprise because he
won the final episode of the season despite never having won the
previous episodes during the season compared to Steph’s 4 winning
titles.

Import, clean, tidy, and organize the viewership dataset:

``` r
viewers =
  read_csv("data/gbb_datasets/viewers.csv") |>
  janitor::clean_names() |>
  pivot_longer(cols = starts_with("series_"), 
               names_to = "series",
               values_to = "viewership") |>
  mutate(series = as.numeric(gsub("series_", "", series)))
```

Show the first 10 rows of this dataset:

``` r
head(viewers, 10)
```

    ## # A tibble: 10 × 3
    ##    episode series viewership
    ##      <dbl>  <dbl>      <dbl>
    ##  1       1      1       2.24
    ##  2       1      2       3.1 
    ##  3       1      3       3.85
    ##  4       1      4       6.6 
    ##  5       1      5       8.51
    ##  6       1      6      11.6 
    ##  7       1      7      13.6 
    ##  8       1      8       9.46
    ##  9       1      9       9.55
    ## 10       1     10       9.62

Calculate the average viewership in Season 1 and Season 5:

``` r
viewers |>
  filter(series == 1) |>
  summarise(`average viewership in season 1` = mean(viewership, na.rm = TRUE))
```

    ## # A tibble: 1 × 1
    ##   `average viewership in season 1`
    ##                              <dbl>
    ## 1                             2.77

``` r
viewers |>
  filter(series == 5) |>
  summarise(`average viewership in season 5` = mean(viewership, na.rm = TRUE))
```

    ## # A tibble: 1 × 1
    ##   `average viewership in season 5`
    ##                              <dbl>
    ## 1                             10.0

- The average viewership in Season 1 is **2.77**.

- The average viewership in Season 5 is **10.04**.
